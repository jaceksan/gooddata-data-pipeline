###################
# Templates
.dbt:
  extends:
    - .base
  # We build a custom image on top of the official dbt image in this pipeline
  # It contains dbt itself, all required dbt plugins and PoC of dbt-gooddata plugin (not yet in pypi, TODO)
  image:
    name: "$DBT_CUSTOM_IMAGE"
    entrypoint: [""]
  stage: transform
  before_script:
    - cd $SRC_DATA_PIPELINE
    # dbt packages are installed during build of docker image to workdir
    - ln -s ${IMAGES_WORKDIR}/dbt_packages dbt_packages
    - if [ "$FULL_REFRESH" == "true" ]; then export FR_ARG="--full-refresh"; else export FR_ARG=""; fi
  script:
    - dbt run --profiles-dir $DBT_PROFILE_DIR --profile $DBT_PROFILE --target $ELT_ENVIRONMENT $FR_ARG
    - dbt test --profiles-dir $DBT_PROFILE_DIR --profile $DBT_PROFILE --target $ELT_ENVIRONMENT
    - dbt-gooddata deploy_models $GOODDATA_UPPER_CASE
    # Invalidates GoodData caches
    - dbt-gooddata upload_notification

.dbt_changes:
  changes:
    - $SRC_DATA_PIPELINE/models/**/*
    - $SRC_DATA_PIPELINE/profile/**/*
    - $SRC_DATA_PIPELINE/dbt_project.yml
    - $SRC_DATA_PIPELINE/packages.yml
    - $SRC_DATA_PIPELINE/requirements-dbt.txt
    - $SRC_DATA_PIPELINE/requirements-gooddata.txt
    - $SRC_DATA_PIPELINE/meltano.yml
    - .gitlab-ci/gitlab-ci-transform.yml
    - .gitlab-ci.yml

##########################3
# Jobs

# pre-merge
dbt_dev:
  extends:
    - .dbt
  variables:
    ELT_ENVIRONMENT: !reference [.envs, dev, ELT_ENVIRONMENT]
    SNOWFLAKE_DBNAME: !reference [.envs, dev, SNOWFLAKE_DBNAME]
    GOODDATA_ENVIRONMENT_ID: !reference [.envs, dev, GOODDATA_ENVIRONMENT_ID]
  rules:
    - if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"'
      changes: !reference [.dbt_changes, changes]
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]

# post-merge
dbt_staging:
  extends:
    - .dbt
  variables:
    ELT_ENVIRONMENT: !reference [.envs, staging, ELT_ENVIRONMENT]
    SNOWFLAKE_DBNAME: !reference [.envs, staging, SNOWFLAKE_DBNAME]
    GOODDATA_ENVIRONMENT_ID: !reference [.envs, staging, GOODDATA_ENVIRONMENT_ID]
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.dbt_changes, changes]
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL_STAGING == "true"'
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]

# merge to prod branch
dbt_prod:
  extends:
    - .dbt
  variables:
    ELT_ENVIRONMENT: !reference [.envs, prod, ELT_ENVIRONMENT]
    SNOWFLAKE_DBNAME: !reference [.envs, prod, SNOWFLAKE_DBNAME]
    GOODDATA_ENVIRONMENT_ID: !reference [.envs, prod, GOODDATA_ENVIRONMENT_ID]
  rules:
    - if: '$CI_COMMIT_BRANCH == "prod" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.dbt_changes, changes]
    # The pipeline scheduler triggers only PROD jobs
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL_PROD == "true"'
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]
