###################
# Templates
.extract_load:
  extends:
    - .base
  # We build a custom image on top of the official Meltano image in this pipeline
  # It contains Meltano itself, and all extractors/loaders
  image:
    name: "$MELTANO_CUSTOM_IMAGE"
    entrypoint: [""]
  stage: extract_load
  variables:
    FULL_REFRESH: "${FULL_REFRESH}"
  parallel:
    matrix:
      - MELTANO_SOURCE: tap-github-repo
        TARGET_SCHEMA: "${INPUT_SCHEMA_GITHUB}"
      - MELTANO_SOURCE: tap-github-org
        TARGET_SCHEMA: "${INPUT_SCHEMA_GITHUB}"
      - MELTANO_SOURCE: tap-s3-csv
        TARGET_SCHEMA: "${INPUT_SCHEMA_FAA}"
      - MELTANO_SOURCE: tap-exchangeratehost
        TARGET_SCHEMA: "${INPUT_SCHEMA_EXCHANGERATEHOST}"
# TODO - .meltano is linked to include plugins, artifacts cannot be gathered through links
#  artifacts:
#    paths:
#      - .meltano/logs/**
  before_script:
    - cd $SRC_DATA_PIPELINE
    # Meltano plugins are installed during build of docker image to workdir
    - ln -s ${IMAGES_WORKDIR}/.meltano .meltano
    - if [ "$FULL_REFRESH" == "true" ]; then export FR_ARG="--full-refresh"; else export FR_ARG=""; fi
  script:
    - meltano --environment $ELT_ENVIRONMENT run $MELTANO_SOURCE $MELTANO_TARGET $FR_ARG

.extract_load_snowflake:
  extends:
    - .extract_load
  variables:
    DB_USER: "${SNOWFLAKE_USER}"
    DB_PASS: "${SNOWFLAKE_PASS}"
    DB_WAREHOUSE: "${SNOWFLAKE_WAREHOUSE}"
    DB_ACCOUNT: "${SNOWFLAKE_ACCOUNT}"

.extract_load_vertica:
  extends:
    - .extract_load
  variables:
    DB_HOST: "${VERTICA_HOST}"
    DB_PORT: "${VERTICA_PORT}"
    DB_USER: "${VERTICA_USER}"
    DB_PASS: "${VERTICA_PASS}"
    DB_NAME: "${VERTICA_DBNAME}"
    MELTANO_TARGET: "target-vertica"

.extract_load_changes:
  changes:
    - $SRC_DATA_PIPELINE/meltano.yml
    - $SRC_DATA_PIPELINE/requirements-meltano.txt
    - .gitlab-ci/gitlab-ci-extract-load.yml
    - .gitlab-ci.yml

##########################3
# Jobs

# pre-merge
extract_load_dev:
  extends:
    - .extract_load_snowflake
  variables:
    ELT_ENVIRONMENT: $DEV_ELT_ENVIRONMENT
    DB_NAME: $DEV_SNOWFLAKE_DBNAME
  rules:
    - if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"'
      changes: !reference [.extract_load_changes, changes]
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]

# pre-merge Cloud
extract_load_cloud_dev:
  extends:
    - .extract_load_snowflake
  variables:
    ELT_ENVIRONMENT: $DEV_ELT_ENVIRONMENT
    DB_NAME: $CLOUD_DEV_SNOWFLAKE_DBNAME
  rules:
      - if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"'
        changes: !reference [.extract_load_changes, changes]
      - !reference [.base_rules, rules]
      - !reference [.elt_rules, rules]

# post-merge
extract_load_staging:
  extends:
    - .extract_load_snowflake
  variables:
    ELT_ENVIRONMENT: $STAGING_ELT_ENVIRONMENT
    DB_NAME: $STAGING_SNOWFLAKE_DBNAME
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.extract_load_changes, changes]
    # The pipeline scheduler. Run ELT jobs regularly to load new data.
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL_STAGING == "true"'
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]

# post-merge Cloud
extract_load_cloud_staging:
  extends:
    - .extract_load_snowflake
  variables:
    ELT_ENVIRONMENT: $STAGING_ELT_ENVIRONMENT
    DB_NAME: $CLOUD_STAGING_SNOWFLAKE_DBNAME
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.extract_load_changes, changes]
    # The pipeline scheduler. Run ELT jobs regularly to load new data.
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL_STAGING == "true"'
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]

extract_load_staging_vertica:
  extends:
    - .extract_load_vertica
  variables:
    ELT_ENVIRONMENT: $STAGING_ELT_ENVIRONMENT
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.extract_load_changes, changes]
    # The pipeline scheduler. Run ELT jobs regularly to load new data.
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL_STAGING == "true"'
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]

# merge to prod branch
extract_load_prod:
  extends:
    - .extract_load_snowflake
  variables:
    ELT_ENVIRONMENT: $PROD_ELT_ENVIRONMENT
    DB_NAME: $PROD_SNOWFLAKE_DBNAME
  rules:
    - if: '$CI_COMMIT_BRANCH == "prod" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.extract_load_changes, changes]
    # The pipeline scheduler. Run ELT jobs regularly to load new data.
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL_PROD == "true"'
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]

# merge to prod branch (cloud)
extract_load_cloud_prod:
  extends:
    - .extract_load_snowflake
  variables:
    ELT_ENVIRONMENT: $PROD_ELT_ENVIRONMENT
    DB_NAME: $CLOUD_PROD_SNOWFLAKE_DBNAME
  rules:
    - if: '$CI_COMMIT_BRANCH == "prod" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.extract_load_changes, changes]
    # The pipeline scheduler. Run ELT jobs regularly to load new data.
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL_PROD == "true"'
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]
