stages:
  - build
  - extract_load
  - transform
  - analytics

##########################
# Global variables
variables:
  DBT_PROFILE: "default"
  GOODDATA_MODEL_IDS: "github faa"
  GOODDATA_UPPER_CASE: "--gooddata-upper-case"
  # TODO - store the state in a so-called custom state backend, e.g. in AWS S3, and get rid of PostgreSQL
  # https://docs.meltano.com/concepts/state_backends#aws-s3
  POSTGRES_HOST: "db.bit.io"
  POSTGRES_PORT: 5432
  POSTGRES_USER: "cicd"
  SNOWFLAKE_ACCOUNT: "gooddata"
  SNOWFLAKE_USER: "cicd"
  SNOWFLAKE_WAREHOUSE: "DEMO_WH"
  INPUT_SCHEMA: "cicd_input_stage"
  OUTPUT_SCHEMA: "cicd_output_stage"
  MELTANO_CUSTOM_IMAGE_BASE: "gooddata-data-pipeline-meltano"
  MELTANO_VERSION: "v2.12.0-python3.10"
  MELTANO_CUSTOM_IMAGE: "$CI_REGISTRY_IMAGE/$MELTANO_CUSTOM_IMAGE_BASE:$MELTANO_VERSION"
  DBT_CUSTOM_IMAGE_BASE: "gooddata-data-pipeline-dbt"
  DBT_VERSION: "1.4.1"
  DBT_CUSTOM_IMAGE: "$CI_REGISTRY_IMAGE/$DBT_CUSTOM_IMAGE_BASE:$DBT_VERSION"

.envs:
  # Meltano, dbt and GoodData environments have 1:1 relationship in this demo
  # But, you can design you pipeline in any alternative way, e.g. share 1 data source by multiple GoodData workspaces
  dev:
    ELT_ENVIRONMENT: "cicd_dev"
    POSTGRES_DBNAME: "jaceksan/cicd_dev"
    SNOWFLAKE_DBNAME: "CICD_DEV"
    GOODDATA_ENVIRONMENT_ID: "development"
  staging:
    ELT_ENVIRONMENT: "cicd_staging"
    POSTGRES_DBNAME: "jaceksan/cicd_staging"
    SNOWFLAKE_DBNAME: "CICD_STAGING"
    GOODDATA_ENVIRONMENT_ID: "staging"
  prod:
    ELT_ENVIRONMENT: "cicd_prod"
    POSTGRES_DBNAME: "jaceksan/cicd_prod"
    SNOWFLAKE_DBNAME: "CICD_PROD"
    GOODDATA_ENVIRONMENT_ID: "production"


##########################
# Job templates
##########################
.base:
  image: python:3.10-slim-bullseye

.base_rules:
  rules:
    - if: '$RUN_ALL_JOBS == "true"'
      when: manual

.docker:
  extends:
    - .base
  image: docker:latest
  services:
    - docker:18.09.7-dind # older version that does not need demand TLS (see below)
  before_script:
    - docker login -u ${CI_REGISTRY_USER} -p ${CI_REGISTRY_PASSWORD} ${CI_REGISTRY}

.extract_load:
  extends:
    - .base
  # We build a custom image on top of the official Meltano image in this pipeline
  # It contains Meltano itself, and all extractors/loaders
  image:
    name: "$MELTANO_CUSTOM_IMAGE"
    entrypoint: [""]
  stage: extract_load
  variables:
    TAP_GITHUB_AUTH_TOKEN: "$TAP_GITHUB_AUTH_TOKEN"
    MELTANO_DATABASE_URI: "postgresql://$POSTGRES_USER:$POSTGRES_PASS@$POSTGRES_HOST:$POSTGRES_PORT/$POSTGRES_DBNAME?options=-csearch_path%3Dmeltano&sslmode=require"
    MELTANO_TARGET: "target-snowflake"
    # CI_DEBUG_TRACE: "true"
    GIT_STRATEGY: "none"
  artifacts:
    paths:
      - .meltano/logs/**
  before_script:
    - cd /project
  script:
    - meltano --environment $ELT_ENVIRONMENT run tap-github-repo $MELTANO_TARGET tap-github-org $MELTANO_TARGET tap-s3-csv $MELTANO_TARGET

.extract_load_changes:
  changes:
    - src/meltano.yml
    - .gitlab-ci.yml

.dbt:
  extends:
    - .base
  # We build a custom image on top of the official dbt image in this pipeline
  # It contains dbt itself, all required dbt plugins and PoC of dbt-gooddata plugin (not yet in pypi, TODO)
  image:
    name: "$DBT_CUSTOM_IMAGE"
    entrypoint: [""]
  stage: transform
  before_script:
    - cd /usr/app
  script:
    - dbt run --profile $DBT_PROFILE --target $ELT_ENVIRONMENT
    - dbt test --profile $DBT_PROFILE --target $ELT_ENVIRONMENT
    - dbt-gooddata deploy_models $GOODDATA_UPPER_CASE
    - dbt-gooddata upload_notification

.dbt_changes:
  changes:
    - src/models/**/*
    - src/profile/**/*
    - src/dbt_project.yml
    - src/packages.yml
    - src/requirements-dbt.txt
    - src/dbt-gooddata/**/*
    - .gitlab-ci.yml

.gooddata:
  extends:
    - .base
  # We build a custom image on top of the official dbt image in this pipeline
  # It contains dbt itself, all required dbt plugins and PoC of dbt-gooddata plugin (not yet in pypi, TODO)
  image:
    name: "$DBT_CUSTOM_IMAGE"
    entrypoint: [""]
  stage: analytics
  before_script:
    - cd /usr/app
    # Compile to generate manifest.json, which is parsed by dbt-gooddata module
    - dbt compile --profile $DBT_PROFILE --target $ELT_ENVIRONMENT
  script:
    - dbt-gooddata deploy_analytics $GOODDATA_UPPER_CASE
    - dbt-gooddata test_insights

.gooddata_changes:
  changes:
    - src/macros/**/*
    - src/models/**/*
    - src/dbt-gooddata/**/*
    - src/gooddata_layouts/**/*
    - .gitlab-ci.yml

#############
# Pre-merge
#############

# Builds
build_meltano:
  extends:
    - .docker
  stage: build
  script:
    - cd "$CI_PROJECT_DIR"
    - docker build
      --push
      --build-arg MELTANO_VERSION=$MELTANO_VERSION 
      -f Dockerfile_meltano 
      -t $MELTANO_CUSTOM_IMAGE .
  rules:
    - if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"'
      changes:
        - src/Dockerfile_meltano
        - src/meltano.yml
        - .gitlab-ci.yml
    - !reference [.base_rules, rules]

build_dbt:
  extends:
    - .docker
  stage: build
  script:
    - cd "$CI_PROJECT_DIR"
    - docker build
      --push
      --build-arg DBT_VERSION=$DBT_VERSION
      -f Dockerfile_dbt
      -t $DBT_CUSTOM_IMAGE .
  rules:
    - if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"'
      changes:
        - src/Dockerfile_dbt
        - src/packages.yml
        - src/dbt_project.yml
        - src/profile/profiles.yml
        - src/models
        - src/macros
        - .gitlab-ci.yml
    - !reference [.base_rules, rules]

# Data pipeline jobs
extract_load_dev:
  extends:
    - .extract_load
  variables:
    ELT_ENVIRONMENT: !reference [.envs, dev, ELT_ENVIRONMENT]
    POSTGRES_DBNAME: !reference [.envs, dev, POSTGRES_DBNAME]
    SNOWFLAKE_DBNAME: !reference [.envs, dev, SNOWFLAKE_DBNAME]
  rules:
    - if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"'
      changes: !reference [.extract_load_changes, changes]
    - !reference [.base_rules, rules]

dbt_dev:
  extends:
    - .dbt
  variables:
    ELT_ENVIRONMENT: !reference [.envs, dev, ELT_ENVIRONMENT]
    POSTGRES_DBNAME: !reference [.envs, dev, POSTGRES_DBNAME]
    SNOWFLAKE_DBNAME: !reference [.envs, dev, SNOWFLAKE_DBNAME]
  rules:
    - if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"'
      changes: !reference [.dbt_changes, changes]
    - !reference [.base_rules, rules]

gooddata_dev:
  extends:
    - .gooddata
  variables:
    ELT_ENVIRONMENT: !reference [.envs, dev, ELT_ENVIRONMENT]
    POSTGRES_DBNAME: !reference [.envs, dev, POSTGRES_DBNAME]
    SNOWFLAKE_DBNAME: !reference [.envs, dev, SNOWFLAKE_DBNAME]
  rules:
    - if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"'
      changes: !reference [.gooddata_changes, changes]
    - !reference [.base_rules, rules]

#############
# Post-merge
#############

extract_load_staging:
  extends:
    - .extract_load
  variables:
    ELT_ENVIRONMENT: !reference [.envs, staging, ELT_ENVIRONMENT]
    POSTGRES_DBNAME: !reference [.envs, staging, POSTGRES_DBNAME]
    SNOWFLAKE_DBNAME: !reference [.envs, staging, SNOWFLAKE_DBNAME]
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.extract_load_changes, changes]
    - !reference [.base_rules, rules]

dbt_staging:
  extends:
    - .dbt
  variables:
    ELT_ENVIRONMENT: !reference [.envs, staging, ELT_ENVIRONMENT]
    POSTGRES_DBNAME: !reference [.envs, staging, POSTGRES_DBNAME]
    SNOWFLAKE_DBNAME: !reference [.envs, staging, SNOWFLAKE_DBNAME]
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.dbt_changes, changes]
    - !reference [.base_rules, rules]

gooddata_staging:
  extends:
    - .gooddata
  variables:
    ELT_ENVIRONMENT: !reference [.envs, staging, ELT_ENVIRONMENT]
    POSTGRES_DBNAME: !reference [.envs, staging, POSTGRES_DBNAME]
    SNOWFLAKE_DBNAME: !reference [.envs, staging, SNOWFLAKE_DBNAME]
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.gooddata_changes, changes]
    - !reference [.base_rules, rules]

##########################
# Post-merge - PROD
##########################
extract_load_prod:
  extends:
    - .extract_load
  variables:
    ELT_ENVIRONMENT: !reference [.envs, prod, ELT_ENVIRONMENT]
    POSTGRES_DBNAME: !reference [.envs, prod, POSTGRES_DBNAME]
    SNOWFLAKE_DBNAME: !reference [.envs, prod, SNOWFLAKE_DBNAME]
  rules:
    - if: '$CI_COMMIT_BRANCH == "prod" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.extract_load_changes, changes]
    # The pipeline scheduler triggers only PROD jobs
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL == "true"'
    - !reference [.base_rules, rules]

dbt_prod:
  extends:
    - .dbt
  variables:
    ELT_ENVIRONMENT: !reference [.envs, prod, ELT_ENVIRONMENT]
    POSTGRES_DBNAME: !reference [.envs, prod, POSTGRES_DBNAME]
    SNOWFLAKE_DBNAME: !reference [.envs, prod, SNOWFLAKE_DBNAME]
  rules:
    - if: '$CI_COMMIT_BRANCH == "prod" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.dbt_changes, changes]
    # The pipeline scheduler triggers only PROD jobs
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL == "true"'
    - !reference [.base_rules, rules]

gooddata_prod:
  extends:
    - .gooddata
  variables:
    ELT_ENVIRONMENT: !reference [.envs, prod, ELT_ENVIRONMENT]
    POSTGRES_DBNAME: !reference [.envs, prod, POSTGRES_DBNAME]
    SNOWFLAKE_DBNAME: !reference [.envs, prod, SNOWFLAKE_DBNAME]
  rules:
    - if: '$CI_COMMIT_BRANCH == "prod" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.gooddata_changes, changes]
    - !reference [.base_rules, rules]
